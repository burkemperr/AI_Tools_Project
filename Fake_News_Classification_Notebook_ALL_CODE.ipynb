{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84bafa0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "from scipy.spatial import distance\n",
    "from scipy import stats\n",
    "import math\n",
    "from statistics import mode\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd4fda4a",
   "metadata": {},
   "source": [
    "# Reading Dataset #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "020ce6e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LAW ENFORCEMENT ON HIGH ALERT Following Threat...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>UNBELIEVABLE! OBAMA’S ATTORNEY GENERAL SAYS MO...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bobby Jindal, raised Hindu, uses story of Chri...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SATAN 2: Russia unvelis an image of its terrif...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>About Time! Christian Group Sues Amazon and SP...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  label\n",
       "0  LAW ENFORCEMENT ON HIGH ALERT Following Threat...      1\n",
       "2  UNBELIEVABLE! OBAMA’S ATTORNEY GENERAL SAYS MO...      1\n",
       "3  Bobby Jindal, raised Hindu, uses story of Chri...      0\n",
       "4  SATAN 2: Russia unvelis an image of its terrif...      1\n",
       "5  About Time! Christian Group Sues Amazon and SP...      1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"/home/sterritts/WELFake_Dataset.csv\")\n",
    "data = data.dropna()\n",
    "data = data.drop(columns = [\"text\"])\n",
    "data = data.drop(columns = [\"Unnamed: 0\"])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "905a38aa",
   "metadata": {},
   "source": [
    "# KNN #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34f50a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def run_KNN(k, X_train, y_train, X_test, y_test):\n",
    "    knn = KNeighborsClassifier(n_neighbors = k)\n",
    "    knn.fit(X_train, y_train)\n",
    "    y_pred = knn.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae6140e",
   "metadata": {},
   "source": [
    "# Decision Tree Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e70b4605",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn import metrics\n",
    "\n",
    "def run_Tree(depth, X_train, y_train, X_test, y_test):\n",
    "    mod_dt = DecisionTreeClassifier(max_depth = depth, random_state = 1)\n",
    "    mod_dt.fit(X_train, y_train)\n",
    "    y_pred = mod_dt.predict(X_test)\n",
    "#     plt.figure(figsize = (5,5), dpi = 200)\n",
    "#     plot_tree(mod_dt,feature_names=X_train.,class_names=mod_dt.classes_, filled = True)\n",
    "#     plt.show()\n",
    "    return (metrics.accuracy_score(y_pred,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "964df84c",
   "metadata": {},
   "source": [
    "# DNN***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4ed76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8ac34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_binary_mlp(X_train, y_train, X_test, y_test, hidden=64, epochs=100):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(hidden, activation='relu', input_shape=(384,)))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(hidden, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid')) # output layer, add hidden layers before this\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.0002)\n",
    "    model.compile(loss=\"binary_crossentropy\",\n",
    "              optimizer=optimizer,\n",
    "              metrics=['accuracy'])\n",
    "    history = model.fit(x=X_train,y=y_train,\n",
    "                        validation_data = (X_test, y_test),\n",
    "                        batch_size=16,epochs=epochs,\n",
    "                        verbose=0)\n",
    "    model.summary()\n",
    "    predicted_probabilities = model.predict(X_train)\n",
    "    predicted_probabilities = np.rint(predicted_probabilities)\n",
    "    acc = 100. * accuracy_score(y_train, predicted_probabilities)\n",
    "    print(\"Accuracy on train set: {:.2f}%\".format(acc))\n",
    "    predicted_probabilities = model.predict(X_test)\n",
    "    predicted_probabilities = np.rint(predicted_probabilities)\n",
    "    acc = 100. * accuracy_score(y_test, predicted_probabilities)\n",
    "    print(\"Accuracy on test set: {:.2f}%\".format(acc))\n",
    "    print(confusion_matrix(y_test, predicted_probabilities))\n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d80d49",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99919607",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "# Features (TF-IDF matrix) and target (labels)\n",
    "X = tf_idf_vectors_matrix  # using full TF-IDF matrix\n",
    "y = data[\"label\"]          # target variable (Fake = 1, Real = 0)\n",
    "\n",
    "# split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# train the model\n",
    "model3 = LogisticRegression(solver='liblinear', multi_class='ovr')\n",
    "model3.fit(X_train, y_train)\n",
    "# Predict on the test set\n",
    "y_pred = model3.predict(X_test)\n",
    "print(\"accuracy: \", accuracy_score(y_test, y_pred))\n",
    "\n",
    "# training accuracy for overfitting\n",
    "y_train_pred = logreg.predict(X_train)\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "print(\"training accuracy: \", train_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91d4b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "# Features (bag of words) and target (labels)\n",
    "labels_vector = data[\"label\"].to_numpy()\n",
    "# Split data into training and testing sets\n",
    "LX_train2, LX_test2, Ly_train2, Ly_test2 = train_test_split(tf_idf_vectors_matrix, labels_vector, test_size=0.2, random_state=0)\n",
    "\n",
    "\n",
    "# Initialize and train the model\n",
    "Lmodel32 = LogisticRegression(solver='liblinear', multi_class='ovr')\n",
    "Lmodel32.fit(LX_train2, Ly_train2)\n",
    "# Predict on the test set\n",
    "Ly_pred2 = Lmodel32.predict(LX_test2)\n",
    "print(\"accuracy: \", accuracy_score(y_test2, y_pred2))\n",
    "\n",
    "# training accuracy for overfitting\n",
    "Ly_train_pred2 = Lmodel32.predict(LX_train2)\n",
    "Ltrain_accuracy2 = accuracy_score(Ly_train2, Ly_train_pred2)\n",
    "print(\"training accuracy: \", Ltrain_accuracy2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ae6d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features (bag of words) and target (labels)\n",
    "labels_vector = data[\"label\"].to_numpy()\n",
    "# Split data into training and testing sets\n",
    "LX_train, LX_test, Ly_train, Ly_test = train_test_split(NFTK_data, labels_vector, test_size=0.2, random_state=0)\n",
    "\n",
    "\n",
    "# Initialize and train the model\n",
    "model32 = LogisticRegression(solver='liblinear', multi_class='ovr')\n",
    "model32.fit(LX_train, Ly_train)\n",
    "# Predict on the test set\n",
    "Ly_pred = model32.predict(LX_test)\n",
    "print(\"accuracy: \", accuracy_score(Ly_test, Ly_pred))\n",
    "\n",
    "# training accuracy for overfitting\n",
    "Ly_train_pred = model32.predict(LX_train)\n",
    "Ltrain_accuracy = accuracy_score(Ly_train, Ly_train_pred)\n",
    "print(\"training accuracy: \", Ltrain_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f863a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features (bag of words) and target (labels)\n",
    "labels_vector = data[\"label\"].to_numpy()\n",
    "# Split data into training and testing sets\n",
    "LX_train2, LX_test2, Ly_train2, Ly_test2 = train_test_split(embedded_titles, labels_vector, test_size=0.2, random_state=0)\n",
    "\n",
    "\n",
    "# Initialize and train the model\n",
    "Lmodel32 = LogisticRegression(solver='liblinear', multi_class='ovr')\n",
    "Lmodel32.fit(LX_train2, Ly_train2)\n",
    "# Predict on the test set\n",
    "Ly_pred2 = Lmodel32.predict(LX_test2)\n",
    "print(\"accuracy: \", accuracy_score(y_test2, y_pred2))\n",
    "\n",
    "# training accuracy for overfitting\n",
    "Ly_train_pred2 = Lmodel32.predict(LX_train2)\n",
    "Ltrain_accuracy2 = accuracy_score(Ly_train2, Ly_train_pred2)\n",
    "print(\"training accuracy: \", Ltrain_accuracy2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "417cf56d",
   "metadata": {},
   "source": [
    "# Bag of Words #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "407c5571",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Method that takes in one headline, preprocesses it \n",
    "#into a list of words, and adds all the new, unique \n",
    "#words into a global frequency dictionary for the \n",
    "#entire dataset.\n",
    "def make_frequency_dict(text):\n",
    "    words = process_sentence(text)\n",
    "    for word in words:\n",
    "        if word in frequency_dict:\n",
    "            frequency_dict[word] += 1\n",
    "        else:\n",
    "            frequency_dict[word] = 1\n",
    "\n",
    "#Preprocessing method that takes in text and puts it \n",
    "#into a list of individual, lowercase words with no \n",
    "#numbers or punctuation.\n",
    "def process_sentence(text):\n",
    "    text = re.sub(r'\\d', '', text)\n",
    "    text = re.sub(r'[^\\w\\s]','',text)\n",
    "    text = text.lower()\n",
    "    words = text.split()\n",
    "    return words\n",
    "\n",
    "#Method that takes in all the headlines of the entire \n",
    "#dataset and the total unique words list and returns \n",
    "#a 2D numpy array that contains a vector for each headline. \n",
    "#Each vector is total_unique_words in length and each index \n",
    "#in the vector represents the count of that word in that \n",
    "#specific headline.\n",
    "def headline_to_vector_fast(headlines, total_unique_words):\n",
    "    word_lists = headlines.apply(process_sentence)\n",
    "    \n",
    "    word_to_index = {word: idk for idk, word in enumerate(total_unique_words)}\n",
    "    \n",
    "    vectors = np.zeros((len(headlines), len(total_unique_words)), dtype = int)\n",
    "    \n",
    "    for i, words in enumerate(word_lists):\n",
    "        for word in words:\n",
    "            if word in word_to_index:\n",
    "                vectors[i, word_to_index[word]] += 1\n",
    "    return vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "261d4c10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words: 36634\n",
      "(71537, 36634)\n",
      "NFTK_data is <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# create frequency dictionary \n",
    "frequency_dict = {}\n",
    "titles_list = data[\"title\"].tolist()\n",
    "for title in titles_list:\n",
    "    make_frequency_dict(title)\n",
    "\n",
    "# extract list of unique words\n",
    "total_unique_words = list(frequency_dict.keys())\n",
    "size = len(total_unique_words)\n",
    "print(\"Number of unique words:\", size)\n",
    "\n",
    "# create input array of dimension: number headlines by unique word vector\n",
    "NFTK_data = headline_to_vector_fast(data[\"title\"], total_unique_words)\n",
    "\n",
    "print(NFTK_data.shape)\n",
    "print(\"NFTK_data is\", type(NFTK_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1591bf76",
   "metadata": {},
   "source": [
    "# TF-IDF #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "82d1e6c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Does the same thing as pocess_sentence, just returns\n",
    "#the text instead of a list containing the words\n",
    "#in the text\n",
    "def preprocess_titles(text):\n",
    "    text = re.sub(r'\\d', '', text)\n",
    "    text = re.sub(r'[^\\w\\s]','',text)\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "processed_titles = data[\"title\"].apply(preprocess_titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0b917d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv=CountVectorizer() \n",
    "count_vector=cv.fit_transform(processed_titles)\n",
    "\n",
    "tfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True)\n",
    "tfidf_transformer.fit(count_vector)\n",
    "\n",
    "# tf-idf scores \n",
    "tf_idf_vectors_matrix=tfidf_transformer.transform(count_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446d0ccb",
   "metadata": {},
   "source": [
    "# Text Embedding (e5-small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7e815e82",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-09 10:56:52.780811: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-12-09 10:56:53.934435: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9373] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-12-09 10:56:53.937149: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-12-09 10:56:54.203828: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1534] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-09 10:56:54.581863: I tensorflow/core/platform/cpu_feature_guard.cc:183] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE3 SSE4.1 SSE4.2 AVX, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "embedding_model = SentenceTransformer(\"intfloat/e5-small-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b5dc740a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(71537,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(71537,)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titles_vector = data[\"title\"].to_numpy()\n",
    "print(titles_vector.shape)\n",
    "labels_vector = data[\"label\"].to_numpy()\n",
    "labels_vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "02e934cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(71537, 384)\n"
     ]
    }
   ],
   "source": [
    "embedded_titles = embedding_model.encode(titles_vector)\n",
    "print(embedded_titles.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f99166",
   "metadata": {},
   "source": [
    "# PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542403bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "scaler = StandardScaler(with_mean=False)\n",
    "tfidf_scaled = scaler.fit_transform(tf_idf_vectors_matrix)\n",
    "pca1 = PCA(n_components=2)\n",
    "components1 = pca1.fit_transform(tfidf_scaled.toarray())\n",
    "\n",
    "# for a data set with n rows, will return an nx2 numpy array\n",
    "print(type(components1))\n",
    "print(components1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761e4a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# we can plot these colored by the labels we calculated to see if\n",
    "# the clusters seem to fit based on the data\n",
    "plt.scatter(components1[:,0], components1[:,1], c = labels_vector)\n",
    "plt.xlabel(\"PC 1\")\n",
    "plt.ylabel(\"PC 2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5560ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2) #determine how many dimensions you want output\n",
    "components = pca.fit_transform(NFTK_data)\n",
    "# for a data set with n rows, will return an nx2 numpy array\n",
    "print(type(components))\n",
    "print(components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc354c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# we can plot these colored by the labels we calculated to see if\n",
    "# the clusters seem to fit based on the data\n",
    "plt.scatter(components[:,0], components[:,1], c = labels_vector)\n",
    "plt.xlabel(\"PC 1\")\n",
    "plt.ylabel(\"PC 2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d802239",
   "metadata": {},
   "source": [
    "# Data Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a631061",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data splittong using bag of words data\n",
    "labels_vector = data[\"label\"].to_numpy()\n",
    "\n",
    "BX_train, BX_test, By_train, By_test = train_test_split(NFTK_data, labels_vector, test_size=0.2, random_state=0)\n",
    "print(BX_train.shape)\n",
    "print(BX_test.shape)\n",
    "print(By_train.shape)\n",
    "print(By_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8dd79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data splitting using TF-IDF data\n",
    "labels_vector = data[\"label\"].to_numpy()\n",
    "\n",
    "TX_train, TX_test, Ty_train, Ty_test = train_test_split(tf_idf_vectors_matrix, labels_vector, test_size=0.2, random_state=0)\n",
    "print(TX_train.shape)\n",
    "print(TX_test.shape)\n",
    "print(Ty_train.shape)\n",
    "print(Ty_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ca1d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data splitting using text embedded data\n",
    "labels_vector = data[\"label\"].to_numpy()\n",
    "\n",
    "EX_train, EX_test, Ey_train, Ey_test = train_test_split(embedded_titles, labels_vector[1000:1500], test_size=0.2, random_state=42)\n",
    "print(EX_train.shape)\n",
    "print(EX_test.shape)\n",
    "print(Ey_train.shape)\n",
    "print(Ey_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd9f870",
   "metadata": {},
   "source": [
    "The data being run was calling the various running methods in for loops using  specificed split data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
